# Statistic_Evaluation_Project

As large language models (LLMs) are becoming heavily integrated into almost all of our everyday applications, concerns such as their ability to make correct ethical decisions have grown. This project evaluates the ethical reasoning capabilities of three different LLMs: Chat-GPT, Mistral, and Gemini, comparing their responses to 5 different data sets that all include human-labeled ethical scenarios. Using a data set structured around five ethical dilemma categories (commonsense, deontology, justice, utilitarianism, and virtue ethics), we evaluated each model's judgments in 500 scenarios per category through a binary evaluation. Through statistical analysis, including accuracy measurement, sample size, McNemarâ€™s test, and Chi-square test, we found significant performance differences between the models, with Gemini and Mistral generally outperforming Chat-GPT, especially in justice and virtue ethics. However, no model dominated across all categories. Behavioral analysis revealed that Mistral tends to default to negative judgments when uncertain, while Chat-GPT leans toward positive labeling. These findings express the need to match LLM choice to the specific ethical context of use and highlight the importance of targeted benchmarking when evaluating AI alignment with human moral reasoning.

To use our dataset: download and open the main folder of the project. Run the main jupyter notebook (.ipynb) file to access the project and all of its statistical analysis.
